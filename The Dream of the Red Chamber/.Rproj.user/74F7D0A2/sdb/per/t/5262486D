{
    "collab_server" : "",
    "contents" : "---\ntitle: \"红楼梦文本分析\"\nauthor: \"带土\"\ndate: \"2016年11月6日\"\noutput: \n    html_document: \n      toc: true\n      toc_float: true\n      code_folding: hide\n      highlight: zenburn\n---\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE,\n                      fig.width = 9.5,fig.height = 6)\noptions(stringsAsFactors = FALSE,scipen = 99)\nrm(list=ls());gc()\n\n## 加载所需要的包####\nlibrary(jiebaR)\nlibrary(tm)\nlibrary(readr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(wordcloud2)\nlibrary(wordcloud)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(scatterplot3d)\nlibrary(plotly)\nlibrary(fastcluster)\nlibrary(lda)\nlibrary(LDAvis)\nlibrary(igraph)\nlibrary(ggraph)\n## R Markdown\n```\n# 介绍\n《红楼梦》文本分析以及关系网络的挖掘。\n\n该工作为大四上学期《数据挖掘》这门课程的一个作业，在这里做了一些练习，相互交流，也希望能取得好成绩。\n\n在这里主要进行和挖掘了如下内容：\n\n1. 《红楼梦》数据的准备、预处理、分词等\n\n2. 《红楼梦》各个章节的字数、词数、段落等相关方面的关系\n\n3. 《红楼梦》整体词频和词云的展示\n\n4. 《红楼梦》各个章节的聚类分析并可视化，主要进行了根据IF-IDF的系统聚类和根据词频的LDA主题模型聚类\n\n5. 《红楼梦》中关系网络的探索，主要探索了各个章节的关系图和人物关系网路图\n\n# 1. 《红楼梦》数据的准备、预处理、分词等\n\n```{r}\n## 读取停用词\nfilename <- \"./数据/我的红楼梦停用词.txt\"\nmystopwords <- readLines(filename)\n## 读取红楼梦\nfilename <-\"./数据/红楼梦UTF82.txt\"\nRed_dream <- readLines(filename,encoding='UTF-8')\n## 将读入的文档分章节####\n#去除空白行\nRed_dream <- Red_dream[!is.na(Red_dream)]\n# Red_dream <- as.vector(Red_dream)\n# Red_dream[is.na(Red_dream)]\n## 删除卷数据\njuan <- grep(Red_dream,pattern = \"^第+.+卷\")\nRed_dream <- Red_dream[(-juan)]\n## 找出每一章节的头部行数和尾部行数\n## 每一章节的名字\nRed_dreamname <- data.frame(name = Red_dream[grep(Red_dream,pattern = \"^第+.+回\")],\n                            chapter = 1:120)\n## 处理章节名\nnames <- data.frame(str_split(Red_dreamname$name,pattern = \" \",simplify =TRUE))\nRed_dreamname$chapter2 <- names$X1\nRed_dreamname$Name <- apply(names[,2:3],1,str_c,collapse = \",\")\n## 每章的开始行数\nRed_dreamname$chapbegin<- grep(Red_dream,pattern = \"^第+.+回\")\n## 每章的结束行数\nRed_dreamname$chapend <- c((Red_dreamname$chapbegin-1)[-1],length(Red_dream))\n## 每章的段落长度\nRed_dreamname$chaplen <- Red_dreamname$chapend - Red_dreamname$chapbegin\n## 每章的内容\nfor (ii in 1:nrow(Red_dreamname)) {\n  ## 将内容使用句号连接\n  chapstrs <- str_c(Red_dream[(Red_dreamname$chapbegin[ii]+1):Red_dreamname$chapend[ii]],collapse = \"\")\n  ## 剔除不必要的空格\n  Red_dreamname$content[ii] <- str_replace_all(chapstrs,pattern = \"[[:blank:]]\",replacement = \"\")\n}\n## 每章节的内容\ncontent <- Red_dreamname$content\nRed_dreamname$content <- NULL\n## 计算每章有多少个字\nRed_dreamname$numchars <- nchar(content)\n\n\n##-----------------------------------------------------------------------\n## 对红楼梦进行分词####\nRed_fen <- jiebaR::worker(type = \"mix\",user = \"./数据/红楼梦词典.txt\")\nFen_red <- apply_list(as.list(content),Red_fen)\n## 去除停用词,使用并行的方法\nlibrary(parallel)\ncl <- makeCluster(4)\nFen_red <- parLapply(cl = cl,Fen_red, filter_segment,filter_words=mystopwords)\nstopCluster(cl)\n# Fen_red <- lapply(Fen_red, filter_segment,filter_words=mystopwords)\n## 每章节最终有多少个词\nFen_red2 <- lapply(Fen_red, unique) #去重\nRed_dreamname$wordlen <- unlist(lapply(Fen_red2,length))\n## 添加分组变量，前80章为1组，后40章为2组\nRed_dreamname$Group <- factor(rep(c(1,2),times = c(80,40)),\n                              labels = c(\"前80章\",\"后40章\"))\n\n\n\n\n```\n## 查看第十章的分词的到的结果\n```{r,echo = TRUE}\n# 第10章的内容\ncontent[[10]]\n# 第10章的分词结果的抽样\nFen_red[[10]][1:100]\n# 第10章分词后的词长\nlength(Fen_red[[10]])\n\n```\n我们可以看出，该段分词后一共有1027个词语，并且给出了一些示例\n\n## 查看每个章节的一些信息\n```{r,echo = TRUE}\nhead(Red_dreamname)\n\n```\n我们这里按照章节整理了一些相关的信息，主要有章节名、内容、段落数、字数、词数等信息\n\n# 2. 《红楼梦》各个章节的字数、词数、段落等相关方面的关系\n## 查看字数、词数、段落等信息的可视化\n```{r}\n## 对每章的内容进行探索分析####\n## 对相关章节进行分析\n## 每章节的段落长度\np1 <- ggplot(Red_dreamname,aes(x = chapter,y = chaplen)) +\n  theme_bw(base_family = \"STKaiti\",base_size = 10) +\n  geom_point(colour = \"red\",size = 1) +\n  geom_line() +\n  geom_text(aes(x = 25,y = 0.9*max(Red_dreamname$chaplen)),\n            label=\"前80章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_text(aes(x = 100,y = 0.9*max(Red_dreamname$chaplen)),\n            label=\"后40章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_vline(xintercept = 80.5,colour = \"blue\") +\n  labs(x = \"章节\",y = \"段数\",title = \"《红楼梦》每章段数\")\n## 每章节的字数\np2 <- ggplot(Red_dreamname,aes(x = chapter,y = numchars)) +\n  theme_bw(base_family = \"STKaiti\",base_size = 10) +\n  geom_point(colour = \"red\",size = 1) +\n  geom_line() +\n  geom_text(aes(x = 25,y = 0.9*max(Red_dreamname$numchars)),\n            label=\"前80章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_text(aes(x = 100,y = 0.9*max(Red_dreamname$numchars)),\n            label=\"后40章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_vline(xintercept = 80.5,colour = \"blue\") +\n  labs(x = \"章节\",y = \"字数\",title = \"《红楼梦》每章字数\")\n\np3 <- ggplot(Red_dreamname,aes(x = chapter,y = wordlen)) +\n  theme_bw(base_family = \"STKaiti\",base_size = 10) +\n  geom_point(colour = \"red\",size = 1) +\n  geom_line() +\n  geom_text(aes(x = 25,y = 0.9*max(Red_dreamname$wordlen)),\n            label=\"前80章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_text(aes(x = 100,y = 0.9*max(Red_dreamname$wordlen)),\n            label=\"后40章\",family = \"STKaiti\",colour = \"Red\") +\n  geom_vline(xintercept = 80.5,colour = \"blue\") +\n  labs(x = \"章节\",y = \"词数\",title = \"《红楼梦》每章词数\")\n## 绘制每一章节的平行坐标图\np4 <- ggparcoord(Red_dreamname,columns = 7:9,scale = \"center\",\n                 groupColumn = \"Group\",showPoints = TRUE,\n                 title = \"《红楼梦》\") +\n  theme_bw(base_family = \"STKaiti\",base_size = 10) +\n  theme(legend.position =  \"bottom\",axis.title.x = element_blank()) +\n  scale_x_discrete(labels = c(\"断落数\",\"字数\",\"词数\")) +\n  ylab(\"中心化数据大小\")\n  \ngridExtra::grid.arrange(p1,p2,p3,p4,ncol = 2)\n\n```\n\n上面的四幅图分别为《红楼梦》中，每个章节的段落数、字数、词数、三者的平行坐标图\n\n从这些相互之间的关系，可以看出，前80章和后40章还是有一些差异的\n\n## 查看段落数、字数、词数三维散点图\n```{r}\n## 对三个变量绘制三散点图，\npar(family = \"STKaiti\",mfcol = c(1,1),cex = 1)\ncolor <- rep(c(\"red\",\"blue\"),times = c(80,40))\npchs <- rep(c(21,22),times = c(80,40))\nscatterplot3d(x =Red_dreamname$chaplen,y = Red_dreamname$numchars,\n              z=Red_dreamname$wordlen,color = color,pch = pchs,\n              xlab=\"断落数\", ylab=\"字数\", zlab=\"词数\",scale.y=1,\n              angle=30,main = \"《红楼梦》\")\nlegend(\"topleft\", inset=.05,      # location and inset\n       bty=\"n\", cex=1,              # suppress legend box, shrink text 50%\n       title=\"章节\",\n       legend = c(\"前80章\",\"后40章\"),\n       pch = c(21,22),\n       col = c(\"red\",\"blue\"))\n\n```\n\n## 查看段落数、字数、词数可交互散点图\n```{r}\n## 对三个变量绘制三散点图，\n## 可交互三维散点图\nplot_ly(Red_dreamname, x = ~chaplen, y = ~numchars, z = ~wordlen) %>% \n  add_markers(color = ~Group,text = ~paste(\"Name: \", name)) %>%\n  layout(title = \"《红楼梦》\")\n```\n\n从三维散点图中可以清晰的看出，三者的空间关系，前80章更加分散，后40章更加的集中\n\n## 查看段落数、字数、词数矩阵散点图\n```{r}\n##矩阵散点图\nRed_dreamname_mat <- Red_dreamname[c(\"chaplen\",\"numchars\",\"wordlen\",\"Group\")]\nnames(Red_dreamname_mat) <- c(\"断落数\",\"字数\",\"词数\",\"章节\")\nggscatmat(Red_dreamname_mat,columns = c(\"断落数\",\"字数\",\"词数\"),color = \"章节\") +\n  theme_bw(base_family = \"STKaiti\") +\n  ggtitle(\"《红楼梦》\")\n```\n从散点矩阵图中可以看出三个变量的分布和相关关系，并且给出了前80章和后40章的相关性大小。\n\n\n# 3. 《红楼梦》整体词频和词云的展示\n\n## 构建文档－词项频数矩阵\n```{r}\n## 词频统计##-----------------------------------------------------------\n## 1:构建文档－词项频数矩阵\ncorpus <- Corpus(VectorSource(Fen_red))\nRed_dtm <- DocumentTermMatrix(corpus,control = list(wordLengths=c(1,Inf)))\nRed_dtm\n## 一共有4万多个词\n```\n我们可以查看一共有120条文档，4万多个词项\n\n## 词频统计\n\n### 频数统计\n```{r}\n## 2:词频统计\nword_freq <- sort(colSums(as.matrix(Red_dtm)),decreasing = TRUE)\nword_freq <- data.frame(word = names(word_freq),freq=word_freq,row.names = NULL)\nword_freq$word <- as.factor(word_freq$word)\n```\n```{r,echo=TRUE}\n## 2:词频统计\nhead(word_freq)\n```\n### 绘制词频图\n```{r,echo=TRUE}\n## 绘制词频图\nnn <- 250\nsum(word_freq$freq>=nn)\n```\n```{r}\n## 绘制词频图\nword_freq[word_freq$freq >= nn,] %>%\n  ggplot(aes(x = word,y = freq)) +\n  theme_bw(base_size = 12,base_family = \"STKaiti\") +\n  geom_bar(stat = \"identity\",fill= \"red\",colour = \"lightblue\",alpha = 0.6) +\n  scale_x_discrete() +\n  theme(axis.text.x = element_text(angle = 75,hjust = 1,size = 10)) +\n  labs(x = \"词项\",y = \"频数\",title = \"《红楼梦》词频图\")\n```\n\n我们绘制了出现频率大于250的一些词（一共有69个）的频频直方图，可以发现不同词之间出现频率的差异\n\n## 词云的绘制\n```{r,echo=TRUE}\n## 词云\nsum(word_freq$freq>=60)\n```\n一共有390个词的频数大于60\n### 静态词云\n```{r}\n### 静态词云\nlayout(matrix(c(1, 2), nrow=2), heights=c(0.4, 4))\npar(mar=rep(0, 4),family = \"STKaiti\")\nplot.new()\ntext(x=0.5, y=0.3, \"红楼梦词云\\nMin=60\")\nwordcloud(words = word_freq$word, freq = word_freq$freq,\n          scale = c(4,0.8),min.freq = 60,random.order=FALSE,\n          family = \"STKaiti\",colors = brewer.pal(8,\"Dark2\"))\n\n```\n\n### 可交互词云\n\n```{r}\n## 动态词云\ndata.frame(word_freq[word_freq$freq>60,]) %>%\n  wordcloud2(color = 'random-dark',backgroundColor = \"whirt\",\n             shape = 'star' )\n\n```\n\n\n\n# 4. 《红楼梦》各个章节的聚类分析并可视化\n\n## 根据TF－IDF矩阵进行系统聚类\n\n### 构建文档－词项TF-IDF矩阵并去稀疏处理\n```{r}\n## 对每章节进行聚类分析####\n## 1:构建文档－词项tf-IDF矩阵\ncorpus2 <- Corpus(VectorSource(Fen_red))\nRed_dtm_tfidf <- DocumentTermMatrix(corpus2,control = list(wordLengths=c(1,Inf),\n                                                    weighting = weightTfIdf))\nRed_dtm_tfidf\n## 一共有4万多个词\n## 降低tfidf矩阵的稀疏度\nRed_dtm_tfidfr <- removeSparseTerms(Red_dtm_tfidf,0.95)\nRed_dtm_tfidfr\n## 只留下了3000多个关键的字\n```\n最终处理后可以的到只剩3千多个重要的关键词\n\n### 聚类为6类时\n```{r,fig.width = 9.5,fig.height = 6}\nRed_dtm_tfidfr_mat <- as.matrix(Red_dtm_tfidfr)\n## 文本间的距离度量为夹角余弦距离\nRed_dtm_tfidfr_dist <- proxy::dist(Red_dtm_tfidfr_mat,method =\"cosine\")\n\n## 系统聚类，聚为两类\nk = 6\nRed_clust <- hclust(d = Red_dtm_tfidfr_dist,method = \"average\")\nRed_clust$labels <- Red_dreamname$chapter2\n## 可视化绘图\npar(family = \"STKaiti\",cex = 0.6)\nplot(Red_clust,\n     main = '红楼梦章节聚类\\nmethod = average',\n     xlab = '', ylab = '', sub = '')\ngroups <- cutree(Red_clust, k=k)   # \"k=\" defines the number of clusters you are using   \nrect.hclust(Red_clust, k=k, border=\"red\") # draw dendogra\n## 每组有多少章\ntable(groups)\n```\n\n### 聚类为5类时\n```{r,fig.width = 9.5,fig.height = 6}\nk = 5\nRed_clust <- hclust(d = Red_dtm_tfidfr_dist,method = \"ward.D2\")\nRed_clust$labels <- Red_dreamname$chapter2\n## 可视化绘图\npar(family = \"STKaiti\",cex = 0.6)\nplot(Red_clust,\n     main = '红楼梦章节聚类\\nmethod = word.D2',\n     xlab = '', ylab = '', sub = '')\ngroups <- cutree(Red_clust, k=k)   # \"k=\" defines the number of clusters you are using   \nrect.hclust(Red_clust, k=k, border=\"red\") # draw dendogra\n## 每组有多少章\ntable(groups)\n\n```\n\n\n## 使用LDA模型挖掘主题\n\n因为该模型会出的结果在Markdown中不能很好的可视化，所以在另一个单独的文件中展示\n\n\n\n\n# 5. 《红楼梦》中关系网络的探索\n\n## 章节的联系探索\n\n如果各章之间距离大于0.8，则视为章节之间没有联系，激励越小，联系越大\n```{r}\n# summary(Red_dtm_tfidfr_dist)\nthreshoud <- 0.8\nRed_dist_cut <- as.matrix(Red_dtm_tfidfr_dist)\nfor (ii in 1:dim(Red_dist_cut)[1]) {\n  for (kk in 1:dim(Red_dist_cut)[2]) {\n    ## 距离大于的则没有连接\n    aa <- Red_dist_cut[ii,kk]\n    ## 数值越小权重越大\n    aa <- ifelse(aa >=threshoud,0,aa)\n    aa <- abs(aa - threshoud)\n    aa <- ifelse(aa < threshoud,aa+threshoud,0)\n    Red_dist_cut[ii,kk] <- aa\n  }\n}\n# # plot(as.vector(Red_dist_cut))\n# # names(Red_dist_cut) <- Red_dreamname$chapter2\nrow.names(Red_dist_cut) <- Red_dreamname$chapter\n## ---------------------------------------------------------\n\n# build a graph from the above matrix\ng <- graph.adjacency(Red_dist_cut, weighted=T, mode = \"undirected\")\n# remove loops\ng <- simplify(g)\n# set labels and degrees of vertices\nV(g)$label <- row.names(Red_dist_cut)\nV(g)$degree <- degree(g)\n\n## 绘制每章节的网络关系图\nset.seed(3952)\npar(family =\"STKaiti\",cex = 1)\nlayout1 <- layout.kamada.kawai(g)\n# plot(g, layout=layout1)\n## 美化图形\nV(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree) +0.4\nV(g)$label.color <- rgb(0, 0, .2, .8)\nV(g)$frame.color <- NA\negam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)\nE(g)$color <- rgb(.5, .9, 0, egam)\nE(g)$width <- egam *4\n# plot the graph in layout1\nplot(g, layout=layout1,main = \"《红楼梦》章节的关系\")\n\n```\n该可视化图模型算法为能量布局算法\n\n\n\n\n```{r}\npar(family =\"STKaiti\",cex = 1)\nlayout2 <- layout.sphere(g)\nplot(g, layout=layout2,main = \"《红楼梦》章节的关系\")\n## 文字越大，说明与该章相关的章节数越多\n## 连接的线越粗，说明联系越大\n\n\n```\n文字越大，说明与该章相关的章节数越多,连接的线越粗，说明联系越大\n\n## 人物社交网络\n\n只分析出现频次大于10的人员\n### 关键人物频数查看\n```{r}\n\n## 读取数据\n## 一共有149人出现的频次多余10次\nRed_net <- read.csv(\"./数据/社交网络权重.csv\")\nRed_net[,1:2] <- apply(Red_net[,1:2],2,as.character)\nName_freq <- read.csv(\"./数据/红楼梦人物出现频次.csv\")\nName_freq <- Name_freq[Name_freq$word %in%(union(unique(Red_net$First),\n                                                 unique(Red_net$Second))),]\nName_freq$word <- as.character(Name_freq$word)\n# union(unique(Red_net$First),unique(Red_net$Second))\n\n## 可视化人出现的频次\np1 <- ggplot(Name_freq,aes(x = reorder(word,freq),y = freq)) +\n  theme_bw(base_size = 9,base_family = \"STKaiti\")+\n  geom_bar(stat = \"identity\",position = \"dodge\",fill = \"lightblue\") +\n  theme(axis.text.x = element_text(size = 5,hjust = 1,angle = 90,vjust = 0.5),\n        axis.title.x = element_blank()) +\n  labs(x = \"\",y = \"频数\",title = \"《红楼梦》中关键人物出现次数\")\n\np2 <- ggplot(Name_freq[Name_freq$freq>80,],aes(x = reorder(word,freq),y = freq)) +\n  theme_bw(base_size = 9,base_family = \"STKaiti\")+\n  geom_bar(stat = \"identity\",position = \"dodge\",fill = \"lightblue\") +\n  theme(axis.text.x = element_text(size = 9,hjust = 1,angle = 90,vjust = 0.5)) +\n  labs(x = \"人名\",y = \"频数\")\n\ngrid.arrange(p1,p2,nrow = 2)\n\n\n```\n\n### 根据是否出现在同一章的社交网络\n\n如果两个入伍同时出现在同一章中一次，则两人之间的权重＋1\n\n只分析两人的权重大于10的关系（因为入关节点太多则网络不好查看）\n\n```{r}\n\n## -----------------------------------------------------------------\n## 按照权重1，即章节权重分析人物的社交网络####\n# 1:准备社交网络数据\nchap_net <- Red_net[Red_net$chapweight > 10,c(1,2,3)]\nnames(chap_net) <- c(\"from\",\"to\",\"weight\")\n\nchap_vert <- Name_freq[Name_freq$word %in% as.character(union(unique(chap_net$from),\n                   unique(chap_net$to))),]\n\nchap_net <- graph_from_data_frame(chap_net,directed = FALSE,\n                                  vertices = chap_vert)\nchap_net\n# # chap_net$name <- \"《红楼梦》章节人物关系\"\n# V(chap_net)$media\n# ## 节点数目\n# vcount(chap_net)\n# ## 边的数目\n# ecount(chap_net)\n## 简化网络图\nchap_net <- simplify(chap_net,remove.multiple = TRUE,remove.loops = TRUE,\n                     edge.attr.comb = \"mean\")\n\n## 查看节点的度\ndegrees <- data.frame(name = names(degree(chap_net)),\n                      counts = (degree(chap_net)))\n\nggplot(degrees,aes(x = reorder(name,counts),y = counts)) +\n  theme_bw(base_size = 11,base_family = \"STKaiti\")+\n  geom_bar(stat = \"identity\",position = \"dodge\",fill = \"lightblue\") +\n  theme(axis.text.x = element_text(size = 8,hjust = 1,angle = 90,vjust = 0.5),\n        axis.title.x = element_blank()) +\n  labs(x = \"人名\",y = \"节点的度\",title = \"《红楼梦》\")\n\n# ## 判断事否为联通图\n# is.connected(chap_net)\n# \n# ## 计算图的直径\n# diameter(chap_net,directed = FALSE)\n\n## \nset.seed(1234)\npar(cex = 0.8,family = \"STKaiti\")\n## 设置图层\nlayout1 <- layout.lgl(chap_net)\nlayout2 <- layout.kamada.kawai(chap_net)\nlayout3 <- layout.reingold.tilford(chap_net)\nlayout4 <- layout.fruchterman.reingold(chap_net)\n\n#V(chap_net)$size <- Name_freq$freq/10\n## 设置节点的字体\nV(chap_net)$label.family <- \"STKaiti\"\nE(chap_net)$width <- round(log10(E(chap_net)$weight))*4\negam <- (E(chap_net)$width) / max(E(chap_net)$width)\nE(chap_net)$color <- rgb(1,0.5,0.5,egam)\nV(chap_net)$size <- log(V(chap_net)$freq) * 2.5\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout1,main = \"《红楼梦》根据章节部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout2,main = \"《红楼梦》根据章节部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout3,main = \"《红楼梦》根据章节部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout4,main = \"《红楼梦》根据章节部分人物关系\")\n\n```\n\n通过不同的网络图表现形式课一比较容易的观察人物之间的关系\n\n\n### 根据是否出现在同一段落的社交网络\n\n如果两个入伍同时出现在同一段落中一次，则两人之间的权重＋1\n\n只分析两人的权重大于10的关系（因为入关节点太多则网络不好查看）\n\n```{r}\n\n## -----------------------------------------------------------------\n## 按照权重2，即段落权重分析人物的社交网络####\n# 1:准备社交网络数据\nchap_net <- Red_net[Red_net$duanweight > 10,c(1,2,4)]\nnames(chap_net) <- c(\"from\",\"to\",\"weight\")\n\nchap_vert <- Name_freq[Name_freq$word %in% as.character(union(unique(chap_net$from),\n                                                              unique(chap_net$to))),]\n\nchap_net <- graph_from_data_frame(chap_net,directed = FALSE,\n                                  vertices = chap_vert)\nchap_net\n# chap_net$name <- \"《红楼梦》章节人物关系\"\n# V(chap_net)$media\n# ## 节点数目\nvcount(chap_net)\n## 边的数目\necount(chap_net)\n## 简化网络图\nchap_net <- simplify(chap_net,remove.multiple = TRUE,remove.loops = TRUE,\n                     edge.attr.comb = \"mean\")\n\n## 查看节点的度\ndegrees <- data.frame(name = names(degree(chap_net)),\n                      counts = (degree(chap_net)))\n\nggplot(degrees,aes(x = reorder(name,counts),y = counts)) +\n  theme_bw(base_size = 11,base_family = \"STKaiti\")+\n  geom_bar(stat = \"identity\",position = \"dodge\",fill = \"lightblue\") +\n  theme(axis.text.x = element_text(size = 8,hjust = 1,angle = 90,vjust = 0.5),\n        axis.title.x = element_blank()) +\n  labs(x = \"人名\",y = \"节点的度\",title = \"《红楼梦》\")\n\n## 判断事否为联通图\nis.connected(chap_net)\n\n## 计算图的直径\ndiameter(chap_net,directed = FALSE)\n\n## \nset.seed(1234)\npar(cex = 0.8,family = \"STKaiti\")\n## 设置图层\nlayout1 <- layout.lgl(chap_net)\nlayout2 <- layout.kamada.kawai(chap_net)\nlayout3 <- layout.reingold.tilford(chap_net)\nlayout4 <- layout.fruchterman.reingold(chap_net)\n\n#V(chap_net)$size <- Name_freq$freq/10\n## 设置节点的字体\nV(chap_net)$label.family <- \"STKaiti\"\nE(chap_net)$width <- log10(E(chap_net)$weight) *2\negam <- (E(chap_net)$width) / max(E(chap_net)$width)\nE(chap_net)$color <- rgb(1,0.5,0.5,egam)\nV(chap_net)$size <- log(V(chap_net)$freq) * 2.5\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout1,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout2,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout3,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout4,main = \"《红楼梦》根据段落部分人物关系\")\n```\n\n### 分析关键人物的社交网络\n\n如果两个入伍同时出现在同一段落中一次，则两人之间的权重＋1\n\n只分析两人的权重大于50的关系\n\n```{r}\n## -----------------------------------------------------------------\n## 按照权重2，即段落权重分析人物的社交网络####\n## 分析链接次数较大的人物\n# 1:准备社交网络数据\nchap_net <- Red_net[Red_net$duanweight > 50,c(1,2,4)]\nnames(chap_net) <- c(\"from\",\"to\",\"weight\")\n\nchap_vert <- Name_freq[Name_freq$word %in% as.character(union(unique(chap_net$from),\n                                                              unique(chap_net$to))),]\n\nchap_net <- graph_from_data_frame(chap_net,directed = FALSE,\n                                  vertices = chap_vert)\nchap_net\n# chap_net$name <- \"《红楼梦》章节人物关系\"\n# V(chap_net)$media\n## 节点数目\nvcount(chap_net)\n## 边的数目\necount(chap_net)\n## 简化网络图\nchap_net <- simplify(chap_net,remove.multiple = TRUE,remove.loops = TRUE,\n                     edge.attr.comb = \"mean\")\n\n## 查看节点的度\ndegrees <- data.frame(name = names(degree(chap_net)),\n                      counts = (degree(chap_net)))\n\nggplot(degrees,aes(x = reorder(name,counts),y = counts)) +\n  theme_bw(base_size = 11,base_family = \"STKaiti\")+\n  geom_bar(stat = \"identity\",position = \"dodge\",fill = \"lightblue\") +\n  theme(axis.text.x = element_text(size = 10,hjust = 1,angle = 90,vjust = 0.5),\n        axis.title.x = element_blank()) +\n  labs(x = \"人名\",y = \"节点的度\",title = \"《红楼梦》\")\n\n## 判断事否为联通图\nis.connected(chap_net)\n\n## 计算图的直径\ndiameter(chap_net,directed = FALSE)\n\n## \nset.seed(1234)\npar(cex = 0.8,family = \"STKaiti\")\n## 设置图层\nlayout1 <- layout.lgl(chap_net)\nlayout2 <- layout.kamada.kawai(chap_net)\nlayout3 <- layout.reingold.tilford(chap_net)\nlayout4 <- layout.fruchterman.reingold(chap_net)\n\n#V(chap_net)$size <- Name_freq$freq/10\n## 设置节点的字体\nV(chap_net)$label.family <- \"STKaiti\"\nE(chap_net)$width <- log10(E(chap_net)$weight) *2\negam <- (E(chap_net)$width) / max(E(chap_net)$width)\nE(chap_net)$color <- rgb(1,0.5,0.5,egam)\nV(chap_net)$size <- log(V(chap_net)$freq) * 2.5\nplot(chap_net,layout = layout1,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout2,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout3,main = \"《红楼梦》根据段落部分人物关系\")\npar(cex = 0.8,family = \"STKaiti\")\nplot(chap_net,layout = layout4,main = \"《红楼梦》根据段落部分人物关系\")\n\n\n```\n\n### 动态社交网络\n\n首先分析在全文的所有段落中共同出现频率大于40的网络链接\n\n```{r}\n##-------------------------------------------------------------------\nlibrary(networkD3)\nlibrary(igraph)\n\n# Basic Graph\nchap_net <- Red_net[Red_net$duanweight > 40,c(1,2,4)]\ng <- graph.data.frame(chap_net, directed=F) # raw graph\n\n\n## Make a vertices df\nvertices<-data.frame(\n  name = V(g)$name,\n  group = edge.betweenness.community(g)$membership,\n  betweenness = (betweenness(g,directed=F,normalized=T)*115)+0.1 #so size isn't tiny\n) \n#nb. can also adjust nodesize with `radiusCalculation`\n\n# create indices (indexing needs to be JS format)\nchap_net$source.index = match(chap_net$First, vertices$name)-1\nchap_net$target.index = match(chap_net$Second, vertices$name)-1\n\n\n# supply a edgelist + nodelist\nd3 = forceNetwork(Links = chap_net, Nodes = vertices,\n                  Source = 'source.index', Target = 'target.index',\n                  NodeID = 'name',\n                  Group = 'group', # color nodes by group calculated earlier\n                  charge = -200, # node repulsion\n                  linkDistance = 20,\n                  zoom = T, \n                  opacity = 1,\n                  fontSize=24)\n\nshow(d3)\n\n```\n该动态结果并没有在RMarkdown文件中输出结果，所以可以在其它文件中演示\n\n\n\n\n\n\n\n\n",
    "created" : 1479263823719.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "747545937",
    "id" : "5262486D",
    "lastKnownWriteTime" : 1479263839,
    "last_content_update" : 1479263839580,
    "path" : "~/数据分析/红楼梦文本挖掘/The Dream of the Red Chamber/Red_dream.Rmd",
    "project_path" : "Red_dream.Rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "tempName" : "Untitled2"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}