{
    "collab_server" : "",
    "contents" : "## 红楼梦文本挖掘之数据预处理####\n## 主要用于文本文档的读取和构建\n## 分析与挖掘R中的人物关系\n## 孙玉林；2016年10月31\n\n\n## 如果在每个段落中人物同时出现，则频数权重加1\n\n## 加载所需要的包\nlibrary(jiebaR)\nlibrary(tm)\nlibrary(readr)\nlibrary(stringr)\nlibrary(plyr)\n\n\n## 读取所需要的文件####\n## 读取停用词\nfilename <- \"./数据/我的红楼梦停用词.txt\"\nmystopwords <- readLines(filename)\n## 读取红楼梦\nfilename <-\"./数据/红楼梦UTF82.txt\"\nRed_dream <- readLines(filename,encoding='UTF-8')\n## 读取人名字典\nfilename <-\"./数据/红楼梦人物.txt\"\nRed_man <- readLines(filename,encoding='UTF-8')\nfilename <-\"./数据/红楼梦诗词123.txt\"\ndictionary <- readLines(filename,encoding='UTF-8')\nsum(is.element(Red_man,dictionary))\nRed_man <- Red_man[is.element(Red_man,dictionary)]\n\n\n## 将读入的文档分章节####\n#去除空白行\nRed_dream <- Red_dream[!is.na(Red_dream)]\n## 删除卷数据\njuan <- grep(Red_dream,pattern = \"^第+.+卷\")\nRed_dream <- Red_dream[(-juan)]\n## 找出每一章节的头部行数和尾部行数\n## 每一章节的名字\nRed_dreamname <- data.frame(name = Red_dream[grep(Red_dream,pattern = \"^第+.+回\")],\n                            chapter = 1:120)\n## 处理章节名\nnames <- data.frame(str_split(Red_dreamname$name,pattern = \" \",simplify =TRUE))\nRed_dreamname$chapter2 <- names$X1\nRed_dreamname$Name <- apply(names[,2:3],1,str_c,collapse = \",\")\n## 每章的开始行数\nRed_dreamname$chapbegin<- grep(Red_dream,pattern = \"^第+.+回\")\n## 每章的结束行数\nRed_dreamname$chapend <- c((Red_dreamname$chapbegin-1)[-1],length(Red_dream))\n## 每章的段落长度\nRed_dreamname$chaplen <- Red_dreamname$chapend - Red_dreamname$chapbegin\n## 每章的内容\nfor (ii in 1:nrow(Red_dreamname)) {\n  ## 将内容使用句号连接\n  chapstrs <- str_c(Red_dream[(Red_dreamname$chapbegin[ii]+1):Red_dreamname$chapend[ii]],collapse = \"\")\n  ## 剔除不必要的空格\n  Red_dreamname$content[ii] <- str_replace_all(chapstrs,pattern = \"[[:blank:]]\",replacement = \"\")\n}\n## 每段落的内容\ncontent <- Red_dreamname$content\nRed_dreamname$content <- NULL\n## 计算每章有多少个字\nRed_dreamname$numchars <- nchar(content)\n##-----------------------------------------------------------------------\n## 根据出现在同一章的座位权重\n## 对红楼梦进行分词####\nRed_fen <- jiebaR::worker(type = \"mix\",user = \"./数据/红楼梦词典.txt\")\nFen_red <- apply_list(as.list(content),Red_fen)\n## 去除停用词,使用并行的方法\nlibrary(parallel)\ncl <- makeCluster(4)\nFen_red <- parLapply(cl = cl,Fen_red, filter_segment,filter_words=mystopwords)\nstopCluster(cl)\n## 词频统计##-----------------------------------------------------------\n## 1:构建文档－词项频数矩阵\ncorpus <- Corpus(VectorSource(Fen_red))\nRed_dtm <- DocumentTermMatrix(corpus,control = list(wordLengths=c(1,Inf)))\nRed_dtm\n## 一共有4万多个词\n\n## 2:词频统计\nword_freq <- sort(colSums(as.matrix(Red_dtm)),decreasing = TRUE)\nword_freq <- data.frame(word = names(word_freq),freq=word_freq,row.names = NULL)\n# word_freq$word <- as.factor(word_freq$word)\n## \nword_freq <- word_freq[is.element(word_freq$word,Red_man),]\nsummary(word_freq)\nhist(word_freq$freq,breaks= 200)\nsum(word_freq$freq >10)\n## 只分析出现次数大于10词的人物\nRed_man <- as.character(word_freq$word[word_freq$freq >10])\nRed_man\n## 生成两两人物的所有组合\nRed_mansol <- t(combn(Red_man,2,simplify = FALSE))\nRed_mansol <- plyr::ldply(Red_mansol)\nnames(Red_mansol) <- c(\"First\",\"Second\")\n\n##-------------------------------------------------\n## 判断每个组合在文档中出现的次数\n## 函数\ntimesFre <- function(strss,fencisol){\n  strs <- as.character(strss)\n  # strs\n  aa <- lapply(fencisol,is.element,el = strs)\n  aa <- lapply(aa,sum)\n  aa <- ifelse(aa == 2,1,0)\n  # weight <- sum(aa)\n  return(sum(aa))\n}\n##-------------------------------------------------\ntimesFre(Red_mansol[1,],fencisol = Fen_red)\n\nsystem.time({\n  weights <- apply(Red_mansol[1:100,],1,timesFre,fencisol = Fen_red)\n})\n\n\nsystem.time({\n  weights <- apply(Red_mansol,1,timesFre,fencisol = Fen_red)\n})\n\n\nhist(weights)\n\n# ## 判断每个组合在文档中出现的次数\n# weights <- vector(mode = \"numeric\", length = nrow(Red_mansol))\n# for (ii in 1:nrow(Red_mansol)) {\n#   strs <- as.character(Red_mansol[ii,1:2])\n#   weight <- 0\n#   for(kk in 1:length(Fen_red)){\n#     weight <- ifelse(sum(is.element(strs,Fen_red[[kk]])) == 2,weight+1,weight)\n#   }\n#   weights[ii] <- weight\n# }\n# summary(weights)\n# table(weights)\n\n# weights <- vector(mode = \"numeric\", length = nrow(Red_mansol))\n# # ii <- 5159\n# for (ii in 1:nrow(Red_mansol)) {\n#   strs <- as.character(Red_mansol[ii,1:2])\n#   # strs\n#   aa <- lapply(Fen_red,is.element,el = strs)\n#   aa <- lapply(aa,sum)\n#   aa <- ifelse(aa == 2,1,0)\n#   weights[ii] <- sum(aa)\n# }\n# summary(weights)\n# table(weights)\n\nRed_mansol$chapweight <- weights\n\n\n##-----------------------------------------------------------------------\n## 根据出现在同一断落中的频数权重\n## 对红楼梦进行分词####\nRed_fen <- jiebaR::worker(type = \"mix\",user = \"./数据/红楼梦词典.txt\")\nFen_red <- apply_list(as.list(Red_dream),Red_fen)\n## 去除停用词,使用并行的方法\nlibrary(parallel)\ncl <- makeCluster(4)\nFen_red <- parLapply(cl = cl,Fen_red, filter_segment,filter_words=mystopwords)\nstopCluster(cl)\n\n## 计算权重\n\n# weights <- vector(mode = \"numeric\", length = nrow(Red_mansol))\n# # ii <- 5159\n# for (ii in 1:nrow(Red_mansol)) {\n#   strs <- as.character(Red_mansol[ii,1:2])\n#   # strs\n#   aa <- lapply(Fen_red,is.element,el = strs)\n#   aa <- lapply(aa,sum)\n#   aa <- ifelse(aa == 2,1,0)\n#   weights[ii] <- sum(aa)\n# }\n# summary(weights)\n# table(weights)\n\nweights <- apply(Red_mansol,1,timesFre,fencisol = Fen_red)\n\nhist(weights)\n\nRed_mansol$duanweight <- weights\n\nwrite_csv(Red_mansol,\"./数据/社交网络权重.csv\")\n\n",
    "created" : 1479253580562.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "362942585",
    "id" : "776AB92A",
    "lastKnownWriteTime" : 1478230470,
    "last_content_update" : 1478230470,
    "path" : "~/数据分析/红楼梦文本挖掘/The Dream of the Red Chamber/Red_man_social.R",
    "project_path" : "Red_man_social.R",
    "properties" : {
        "notebook_format" : "html_document",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}