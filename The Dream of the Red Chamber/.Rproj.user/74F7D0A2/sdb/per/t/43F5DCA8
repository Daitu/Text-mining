{
    "collab_server" : "",
    "contents" : "## 红楼梦文本挖掘之数据预处理####\n## 主要用于文本文档的读取和构建\n## LDA模型\n## 孙玉林；2016年10月31\n\n\n## 加载所需要的包\nlibrary(jiebaR)\nlibrary(tm)\nlibrary(readr)\nlibrary(stringr)\nlibrary(lda)\nlibrary(LDAvis)\n\n\n## 读取所需要的文件####\n## 读取停用词\nfilename <- \"./数据/我的红楼梦停用词.txt\"\nmystopwords <- readLines(filename)\n## 读取红楼梦\nfilename <-\"./数据/红楼梦UTF82.txt\"\nRed_dream <- readLines(filename,encoding='UTF-8')\n## 将读入的文档分章节####\n#去除空白行\nRed_dream <- Red_dream[!is.na(Red_dream)]\n## 删除卷数据\njuan <- grep(Red_dream,pattern = \"^第+.+卷\")\nRed_dream <- Red_dream[(-juan)]\n## 找出每一章节的头部行数和尾部行数\n## 每一章节的名字\nRed_dreamname <- data.frame(name = Red_dream[grep(Red_dream,pattern = \"^第+.+回\")],\n                            chapter = 1:120)\n## 处理章节名\nnames <- data.frame(str_split(Red_dreamname$name,pattern = \" \",simplify =TRUE))\nRed_dreamname$chapter2 <- names$X1\nRed_dreamname$Name <- apply(names[,2:3],1,str_c,collapse = \",\")\n## 每章的开始行数\nRed_dreamname$chapbegin<- grep(Red_dream,pattern = \"^第+.+回\")\n## 每章的结束行数\nRed_dreamname$chapend <- c((Red_dreamname$chapbegin-1)[-1],length(Red_dream))\n## 每章的段落长度\nRed_dreamname$chaplen <- Red_dreamname$chapend - Red_dreamname$chapbegin\n## 每章的内容\nfor (ii in 1:nrow(Red_dreamname)) {\n  ## 将内容使用句号连接\n  chapstrs <- str_c(Red_dream[(Red_dreamname$chapbegin[ii]+1):Red_dreamname$chapend[ii]],collapse = \"\")\n  ## 剔除不必要的空格\n  Red_dreamname$content[ii] <- str_replace_all(chapstrs,pattern = \"[[:blank:]]\",replacement = \"\")\n}\n## 每章节的内容\ncontent <- Red_dreamname$content\nRed_dreamname$content <- NULL\n## 计算每章有多少个字\nRed_dreamname$numchars <- nchar(content)\n\n\n##-----------------------------------------------------------------------\n## 对红楼梦进行分词####\nRed_fen <- jiebaR::worker(type = \"mix\",user = \"./数据/红楼梦词典.txt\")\nFen_red <- apply_list(as.list(content),Red_fen)\n## 去除停用词,使用并行的方法\nlibrary(parallel)\ncl <- makeCluster(4)\nFen_red <- parLapply(cl = cl,Fen_red, filter_segment,filter_words=mystopwords)\nstopCluster(cl)\n\n# compute the table of terms:\n## 计算词项的table\nterm.table <- table(unlist(Fen_red))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# 删除词项出现次数较小的词\ndel <- term.table < 10\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\n# now put the documents into the format required by the lda package:\n## 将文本整理为lda所需要的格式\nget.terms <- function(x) {\n  index <- match(x, vocab)\n  index <- index[!is.na(index)]\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(Fen_red, get.terms)\n\n\n# 计算相关数据集的统计特征\nD <- length(documents)  # number of documents \nW <- length(vocab)  # number of terms in the vocab\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document \nN <- sum(doc.length)  # total number of tokens in the data\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus\n## 使用lda模型\n# MCMC and model tuning parameters:\nK <- 10\nG <- 1000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 24 minutes on laptop\n\n## 对模型进行可视化\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\n\nRED_dreamldavis <- list(phi = phi,\n                     theta = theta,\n                     doc.length = doc.length,\n                     vocab = vocab,\n                     term.frequency = term.frequency)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = RED_dreamldavis$phi, \n                   theta = RED_dreamldavis$theta, \n                   doc.length = RED_dreamldavis$doc.length, \n                   vocab = RED_dreamldavis$vocab, \n                   term.frequency = RED_dreamldavis$term.frequency)\n\nserVis(json, out.dir = 'vis')\n\n\n## ------------------------------------------------------\n# MCMC and model tuning parameters:\nK <- 20\nG <- 1000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 24 minutes on laptop\n\n## 对模型进行可视化\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\n\nRED_dreamldavis <- list(phi = phi,\n                        theta = theta,\n                        doc.length = doc.length,\n                        vocab = vocab,\n                        term.frequency = term.frequency)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = RED_dreamldavis$phi, \n                   theta = RED_dreamldavis$theta, \n                   doc.length = RED_dreamldavis$doc.length, \n                   vocab = RED_dreamldavis$vocab, \n                   term.frequency = RED_dreamldavis$term.frequency)\n\nserVis(json, out.dir = 'vis')\n\n",
    "created" : 1479253591397.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3879656803",
    "id" : "43F5DCA8",
    "lastKnownWriteTime" : 1479258130,
    "last_content_update" : 1479258130,
    "path" : "~/数据分析/红楼梦文本挖掘/The Dream of the Red Chamber/Red_LDAvis.R",
    "project_path" : "Red_LDAvis.R",
    "properties" : {
        "notebook_format" : "html_document",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}